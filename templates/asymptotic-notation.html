<h1>Asymptotic Notations</h1>
<h2>What are they?</h2>
<p>Asymptotic Notations are languages that allow us to analyze an algorithm's<br>
running time by identifying its behavior as the input size for the algorithm<br>
increases. This is also known as an algorithm's growth rate. Does the<br>
algorithm suddenly become incredibly slow when the input size grows? Does it<br>
mostly maintain its quick run time as the input size increases? Asymptotic<br>
Notation gives us the ability to answer these questions.</p>
<h2>Are there alternatives to answering these questions?</h2>
<p>One way would be to count the number of primitive operations at different<br>
input sizes. Though this is a valid solution, the amount of work this takes<br>
for even simple algorithms does not justify its use.</p>
<p>Another way is to physically measure the amount of time an algorithm takes to<br>
complete given different input sizes. However, the accuracy and relativity<br>
(times obtained would only be relative to the machine they were computed on)<br>
of this method is bound to environmental variables such as computer hardware<br>
specifications, processing power, etc.</p>
<h2>Types of Asymptotic Notation</h2>
<p>In the first section of this doc, we described how an Asymptotic Notation<br>
identifies the behavior of an algorithm as the input size changes. Let us<br>
imagine an algorithm as a function f, n as the input size, and f(n) being<br>
the running time. So for a given algorithm f, with input size n you get<br>
some resultant run time f(n). This results in a graph where the Y-axis is<br>
the runtime, the X-axis is the input size, and plot points are the resultants<br>
of the amount of time for a given input size.</p>
<p>You can label a function, or algorithm, with an Asymptotic Notation in many<br>
different ways. Some examples are, you can describe an algorithm by its best<br>
case, worst case, or average case. The most common is to analyze an algorithm<br>
by its worst case. You typically don’t evaluate by best case because those<br>
conditions aren’t what you’re planning for. An excellent example of this is<br>
sorting algorithms; particularly, adding elements to a tree structure. The<br>
best case for most algorithms could be as low as a single operation. However,<br>
in most cases, the element you’re adding needs to be sorted appropriately<br>
through the tree, which could mean examining an entire branch. This is<br>
the worst case, and this is what we plan for.</p>
<h3>Types of functions, limits, and simplification</h3>
<pre class="notranslate"><code class="notranslate">Logarithmic Function - log n
Linear Function - an + b
Quadratic Function - an^2 + bn + c
Polynomial Function - an^z + . . . + an^2 + a*n^1 + a*n^0, where z is some 
constant
Exponential Function - a^n, where a is some constant
</code></pre>
<p>These are some fundamental function growth classifications used in<br>
various notations. The list starts at the slowest growing function<br>
(logarithmic, fastest execution time) and goes on to the fastest<br>
growing (exponential, slowest execution time). Notice that as ‘n’<br>
or the input, increases in each of those functions, the result<br>
increases much quicker in quadratic, polynomial, and exponential,<br>
compared to logarithmic and linear.</p>
<p>It is worth noting that for the notations about to be discussed,<br>
you should do your best to use the simplest terms. This means to<br>
disregard constants, and lower order terms, because as the input<br>
size (or n in our f(n) example) increases to infinity (mathematical<br>
limits), the lower order terms and constants are of little to no<br>
importance. That being said, if you have constants that are 2^9001,<br>
or some other ridiculous, unimaginable amount, realize that<br>
simplifying skew your notation accuracy.</p>
<p>Since we want simplest form, lets modify our table a bit...</p>
<pre class="notranslate"><code class="notranslate">Logarithmic - log n
Linear - n
Quadratic - n^2
Polynomial - n^z, where z is some constant
Exponential - a^n, where a is some constant
</code></pre>
<h3>Big-O</h3>
<p>Big-O, commonly written as <strong>O</strong>, is an Asymptotic Notation for the worst<br>
case, or ceiling of growth for a given function. It provides us with an<br>
<em><strong>asymptotic upper bound</strong></em> for the growth rate of the runtime of an algorithm.<br>
Say <code class="notranslate">f(n)</code> is your algorithm runtime, and <code class="notranslate">g(n)</code> is an arbitrary time<br>
complexity you are trying to relate to your algorithm. <code class="notranslate">f(n)</code> is O(g(n)), if<br>
for some real constants c (c &gt; 0) and n<sub>0</sub>, <code class="notranslate">f(n)</code> &lt;= <code class="notranslate">c g(n)</code> for every input size<br>
n (n &gt; n<sub>0</sub>).</p>
<p><em>Example 1</em></p>
<pre class="notranslate"><code class="notranslate">f(n) = 3log n + 100
g(n) = log n
</code></pre>
<p>Is <code class="notranslate">f(n)</code> O(g(n))?<br>
Is <code class="notranslate">3 log n + 100</code> O(log n)?<br>
Let's look to the definition of Big-O.</p>
<pre class="notranslate"><code class="notranslate">3log n + 100 &lt;= c * log n
</code></pre>
<p>Is there some pair of constants c, n<sub>0</sub> that satisfies this for all n &gt; n<sub>0</sub>?</p>
<pre class="notranslate"><code class="notranslate">3log n + 100 &lt;= 150 * log n, n &gt; 2 (undefined at n = 1)
</code></pre>
<p>Yes! The definition of Big-O has been met therefore <code class="notranslate">f(n)</code> is O(g(n)).</p>
<p><em>Example 2</em></p>
<pre class="notranslate"><code class="notranslate">f(n) = 3*n^2
g(n) = n
</code></pre>
<p>Is <code class="notranslate">f(n)</code> O(g(n))?<br>
Is <code class="notranslate">3 * n^2</code> O(n)?<br>
Let's look at the definition of Big-O.</p>
<pre class="notranslate"><code class="notranslate">3 * n^2 &lt;= c * n
</code></pre>
<p>Is there some pair of constants c, n<sub>0</sub> that satisfies this for all n &gt; <sub>0</sub>?<br>
No, there isn't. <code class="notranslate">f(n)</code> is NOT O(g(n)).</p>
<h3>Big-Omega</h3>
<p>Big-Omega, commonly written as <strong>Ω</strong>, is an Asymptotic Notation for the best<br>
case, or a floor growth rate for a given function. It provides us with an<br>
<em><strong>asymptotic lower bound</strong></em> for the growth rate of the runtime of an algorithm.</p>
<p><code class="notranslate">f(n)</code> is Ω(g(n)), if for some real constants c (c &gt; 0) and n<sub>0</sub> (n<sub>0</sub> &gt; 0), <code class="notranslate">f(n)</code> is &gt;= <code class="notranslate">c g(n)</code><br>
for every input size n (n &gt; n<sub>0</sub>).</p>
<h3>Note</h3>
<p>The asymptotic growth rates provided by big-O and big-omega notation may or<br>
may not be asymptotically tight. Thus we use small-o and small-omega notation<br>
to denote bounds that are not asymptotically tight.</p>
<h3>Small-o</h3>
<p>Small-o, commonly written as <strong>o</strong>, is an Asymptotic Notation to denote the<br>
upper bound (that is not asymptotically tight) on the growth rate of runtime<br>
of an algorithm.</p>
<p><code class="notranslate">f(n)</code> is o(g(n)), if for all real constants c (c &gt; 0) and n<sub>0</sub> (n<sub>0</sub> &gt; 0), <code class="notranslate">f(n)</code> is &lt; <code class="notranslate">c g(n)</code><br>
for every input size n (n &gt; n<sub>0</sub>).</p>
<p>The definitions of O-notation and o-notation are similar. The main difference<br>
is that in f(n) = O(g(n)), the bound f(n) &lt;= g(n) holds for <em><strong>some</strong></em><br>
constant c &gt; 0, but in f(n) = o(g(n)), the bound f(n) &lt; c g(n) holds for<br>
<em><strong>all</strong></em> constants c &gt; 0.</p>
<h3>Small-omega</h3>
<p>Small-omega, commonly written as <strong>ω</strong>, is an Asymptotic Notation to denote<br>
the lower bound (that is not asymptotically tight) on the growth rate of<br>
runtime of an algorithm.</p>
<p><code class="notranslate">f(n)</code> is ω(g(n)), if for all real constants c (c &gt; 0) and n<sub>0</sub> (n<sub>0</sub> &gt; 0), <code class="notranslate">f(n)</code> is &gt; <code class="notranslate">c g(n)</code><br>
for every input size n (n &gt; n<sub>0</sub>).</p>
<p>The definitions of Ω-notation and ω-notation are similar. The main difference<br>
is that in f(n) = Ω(g(n)), the bound f(n) &gt;= g(n) holds for <em><strong>some</strong></em><br>
constant c &gt; 0, but in f(n) = ω(g(n)), the bound f(n) &gt; c g(n) holds for<br>
<em><strong>all</strong></em> constants c &gt; 0.</p>
<h3>Theta</h3>
<p>Theta, commonly written as <strong>Θ</strong>, is an Asymptotic Notation to denote the<br>
<em><strong>asymptotically tight bound</strong></em> on the growth rate of runtime of an algorithm.</p>
<p><code class="notranslate">f(n)</code> is Θ(g(n)), if for some real constants c1, c2 and n<sub>0</sub> (c1 &gt; 0, c2 &gt; 0, n<sub>0</sub> &gt; 0),<br>
<code class="notranslate">c1 g(n)</code> is &lt; <code class="notranslate">f(n)</code> is &lt; <code class="notranslate">c2 g(n)</code> for every input size n (n &gt; n<sub>0</sub>).</p>
<p>∴ <code class="notranslate">f(n)</code> is Θ(g(n)) implies <code class="notranslate">f(n)</code> is O(g(n)) as well as <code class="notranslate">f(n)</code> is Ω(g(n)).</p>
<p>Feel free to head over to additional resources for examples on this. Big-O<br>
is the primary notation use for general algorithm time complexity.</p>
<h3>Endnotes</h3>
<p>It's hard to keep this kind of topic short, and you should go<br>
through the books and online resources listed. They go into much greater depth<br>
with definitions and examples. More where x='Algorithms &amp; Data Structures' is<br>
on its way; we'll have a doc up on analyzing actual code examples soon.</p>
<h2>Books</h2>
<ul>
<li><a href="http://www.amazon.com/Algorithms-4th-Robert-Sedgewick/dp/032157351X" rel="nofollow">Algorithms</a></li>
<li><a href="http://www.amazon.com/Algorithm-Design-Foundations-Analysis-Internet/dp/0471383651" rel="nofollow">Algorithm Design</a></li>
</ul>
<h2>Online Resources</h2>
<ul>
<li><a href="http://web.mit.edu/16.070/www/lecture/big_o.pdf" rel="nofollow">MIT</a></li>
<li><a href="https://www.khanacademy.org/computing/computer-science/algorithms/asymptotic-notation/a/asymptotic-notation" rel="nofollow">KhanAcademy</a></li>
<li><a href="http://bigocheatsheet.com/" rel="nofollow">Big-O Cheatsheet</a> - common structures, operations, and algorithms, ranked by complexity.</li>
</ul>
<hr>
<p>contributors:<br>
- ["Jake Prather", "http://github.com/JakeHP"]<br>
- ["Divay Prakash", "http://github.com/divayprakash"]</p>